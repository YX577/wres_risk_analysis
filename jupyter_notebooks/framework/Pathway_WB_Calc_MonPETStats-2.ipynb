{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Monthly PET between Pathways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two pathways are used within the framework.\n",
    "\n",
    "1. H0: null hypothesis of historical weather statistics\n",
    "2. H1: alternative hypothesis of weather statistics extracted from down-scaled, global climate model results.\n",
    "\n",
    "There are four analysis periods\n",
    "\n",
    "1. Data Period: 1981-2010\n",
    "2. Projection Period 1: 2011-2040\n",
    "3. Projection Period 2: 2041-2070\n",
    "4. Projection Period 3: 2071-2100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from math import floor, ceil\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input directories and file roots\n",
    "IN_DIR = r'C:\\Temp\\WG_Test_Out\\Test2'\n",
    "OUT_DIR = r'C:\\Temp\\WG_Test_Out\\Test2\\Processed'\n",
    "OUT_ROOT = \"DC_WGMN2\"\n",
    "IN_H0ROOT1 = \"WB_H0_%s_R\" % OUT_ROOT\n",
    "IN_H1ROOT1 = \"WB_H1_%s_R\" % OUT_ROOT\n",
    "IN_ROOT2 = \"_DF.pickle\"\n",
    "NUMREAL = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time periods\n",
    "TP_DICT = { 1 : [ dt.datetime(1981, 1, 1), dt.datetime(2010, 12, 31)],\n",
    "            2 : [ dt.datetime(2011, 1, 1), dt.datetime(2040, 12, 31)],\n",
    "            3 : [ dt.datetime(2041, 1, 1), dt.datetime(2070, 12, 31)],\n",
    "            4 : [ dt.datetime(2071, 1, 1), dt.datetime(2100, 12, 31)],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OurQs = np.array( [ round( 0.01 * x, 2 ) for x in range(101) ], dtype=np.float32 )\n",
    "NumQs = len( OurQs )\n",
    "NumQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsHdsList = list()\n",
    "for iI in range(NumQs):\n",
    "    cQ = OurQs[iI]\n",
    "    Hdr = \"%d_ptile\" % round(cQ * 100.0)\n",
    "    StatsHdsList.append( Hdr )\n",
    "# end of for\n",
    "StatsHdsList.append( \"Average\" )\n",
    "StatsHdsList.append( \"Variance\" )\n",
    "StatsHdsList.append( \"Skew\" )\n",
    "StatsHdsList.append( \"Kurt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumHdrs = len( StatsHdsList )\n",
    "NumHdrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_In_Hdrs = [ \"Precip_mm\", #0\n",
    "               \"ETo_mm\", #1\n",
    "               \"PET_mm\", #2\n",
    "               \"P-PET_mm\", #3\n",
    "               \"APWL_mm\", #4\n",
    "               \"SM_mm\", #5\n",
    "               \"DelSM_mm\", #6\n",
    "               \"AET_mm\", #7\n",
    "               \"Def_mm\", #8\n",
    "               \"Surp_mm\", #9\n",
    "               \"TotAvail_mm\", #10\n",
    "               \"RO_mm\", #11\n",
    "               \"Detent_mm\", #12\n",
    "               \"Re_mm\", #13\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on realization 500\n",
      "Working on realization 1000\n",
      "Working on realization 1500\n",
      "Working on realization 2000\n",
      "Working on realization 2500\n",
      "Working on realization 3000\n",
      "Working on realization 3500\n",
      "Working on realization 4000\n",
      "Working on realization 4500\n",
      "Working on realization 5000\n",
      "Working on realization 5500\n",
      "Working on realization 6000\n",
      "Working on realization 6500\n",
      "Working on realization 7000\n",
      "Working on realization 7500\n",
      "Working on realization 8000\n",
      "Working on realization 8500\n",
      "Working on realization 9000\n",
      "Working on realization 9500\n",
      "Working on realization 10000\n"
     ]
    }
   ],
   "source": [
    "for rR in range( 1, NUMREAL + 1, 1 ):\n",
    "    if ((rR % 500) == 0):\n",
    "        print(\"Working on realization %d\" % rR)\n",
    "    # end if\n",
    "    InFile = \"%s%d%s\" % ( IN_H0ROOT1, rR, IN_ROOT2 )\n",
    "    InFP = os.path.normpath( os.path.join( IN_DIR, InFile ) )\n",
    "    InDF = pd.read_pickle( InFP, compression='zip' )\n",
    "    if rR <= 1:\n",
    "        # data period\n",
    "        H0PET1 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[1][0]:TP_DICT[1][1]].copy()\n",
    "        H0PET1.columns = [\"R_%d\" % rR]\n",
    "        # Projection Period 1\n",
    "        H0PET2 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[2][0]:TP_DICT[2][1]].copy()\n",
    "        H0PET2.columns = [\"R_%d\" % rR]\n",
    "        # Projection Period 2\n",
    "        H0PET3 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[3][0]:TP_DICT[3][1]].copy()\n",
    "        H0PET3.columns = [\"R_%d\" % rR]\n",
    "        # Projection Period 3\n",
    "        H0PET4 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[4][0]:TP_DICT[4][1]].copy()\n",
    "        H0PET4.columns = [\"R_%d\" % rR]\n",
    "    else:\n",
    "        # data period\n",
    "        tmpH0PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[1][0]:TP_DICT[1][1]].copy()\n",
    "        tmpH0PET.columns = [\"R_%d\" % rR]\n",
    "        H0PET1 = H0PET1.merge( tmpH0PET, how='inner', left_index=True, right_index=True )\n",
    "        # Projection Period 1\n",
    "        tmpH0PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[2][0]:TP_DICT[2][1]].copy()\n",
    "        tmpH0PET.columns = [\"R_%d\" % rR]\n",
    "        H0PET2 = H0PET2.merge( tmpH0PET, how='inner', left_index=True, right_index=True )\n",
    "        # Projection Period 2\n",
    "        tmpH0PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[3][0]:TP_DICT[3][1]].copy()\n",
    "        tmpH0PET.columns = [\"R_%d\" % rR]\n",
    "        H0PET3 = H0PET3.merge( tmpH0PET, how='inner', left_index=True, right_index=True )\n",
    "        # Projection Period 3\n",
    "        tmpH0PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[4][0]:TP_DICT[4][1]].copy()\n",
    "        tmpH0PET.columns = [\"R_%d\" % rR]\n",
    "        H0PET4 = H0PET4.merge( tmpH0PET, how='inner', left_index=True, right_index=True )\n",
    "    # end of if\n",
    "# end of for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFNamer = \"WBMon_H0_PET1_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H0PET1.to_pickle( OFPath, compression='zip' )\n",
    "#H0PET1 = pd.read_pickle( OFPath, compression='zip' )\n",
    "OFNamer = \"WBMon_H0_PET2_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H0PET2.to_pickle( OFPath, compression='zip' )\n",
    "#H0PET2 = pd.read_pickle( OFPath, compression='zip' )\n",
    "OFNamer = \"WBMon_H0_PET3_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H0PET3.to_pickle( OFPath, compression='zip' )\n",
    "#H0PET3 = pd.read_pickle( OFPath, compression='zip' )\n",
    "OFNamer = \"WBMon_H0_PET4_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H0PET4.to_pickle( OFPath, compression='zip' )\n",
    "#H0PET4 = pd.read_pickle( OFPath, compression='zip' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on realization 500\n",
      "Working on realization 1000\n",
      "Working on realization 1500\n",
      "Working on realization 2000\n",
      "Working on realization 2500\n",
      "Working on realization 3000\n",
      "Working on realization 3500\n",
      "Working on realization 4000\n",
      "Working on realization 4500\n",
      "Working on realization 5000\n",
      "Working on realization 5500\n",
      "Working on realization 6000\n",
      "Working on realization 6500\n",
      "Working on realization 7000\n",
      "Working on realization 7500\n",
      "Working on realization 8000\n",
      "Working on realization 8500\n",
      "Working on realization 9000\n",
      "Working on realization 9500\n",
      "Working on realization 10000\n"
     ]
    }
   ],
   "source": [
    "for rR in range( 1, NUMREAL + 1, 1 ):\n",
    "    if ((rR % 500) == 0):\n",
    "        print(\"Working on realization %d\" % rR)\n",
    "    # end if\n",
    "    InFile = \"%s%d%s\" % ( IN_H1ROOT1, rR, IN_ROOT2 )\n",
    "    InFP = os.path.normpath( os.path.join( IN_DIR, InFile ) )\n",
    "    InDF = pd.read_pickle( InFP, compression='zip' )\n",
    "    if rR <= 1:\n",
    "        # data period\n",
    "        H1PET1 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[1][0]:TP_DICT[1][1]].copy()\n",
    "        H1PET1.columns = [\"R_%d\" % rR]\n",
    "        # Projection Period 1\n",
    "        H1PET2 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[2][0]:TP_DICT[2][1]].copy()\n",
    "        H1PET2.columns = [\"R_%d\" % rR]\n",
    "        # Projection Period 2\n",
    "        H1PET3 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[3][0]:TP_DICT[3][1]].copy()\n",
    "        H1PET3.columns = [\"R_%d\" % rR]\n",
    "        # Projection Period 3\n",
    "        H1PET4 = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[4][0]:TP_DICT[4][1]].copy()\n",
    "        H1PET4.columns = [\"R_%d\" % rR]\n",
    "    else:\n",
    "        # data period\n",
    "        tmpH1PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[1][0]:TP_DICT[1][1]].copy()\n",
    "        tmpH1PET.columns = [\"R_%d\" % rR]\n",
    "        H1PET1 = H1PET1.merge( tmpH1PET, how='inner', left_index=True, right_index=True )\n",
    "        # Projection Period 1\n",
    "        tmpH1PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[2][0]:TP_DICT[2][1]].copy()\n",
    "        tmpH1PET.columns = [\"R_%d\" % rR]\n",
    "        H1PET2 = H1PET2.merge( tmpH1PET, how='inner', left_index=True, right_index=True )\n",
    "        # Projection Period 2\n",
    "        tmpH1PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[3][0]:TP_DICT[3][1]].copy()\n",
    "        tmpH1PET.columns = [\"R_%d\" % rR]\n",
    "        H1PET3 = H1PET3.merge( tmpH1PET, how='inner', left_index=True, right_index=True )\n",
    "        # Projection Period 3\n",
    "        tmpH1PET = InDF[[WB_In_Hdrs[2]]].loc[TP_DICT[4][0]:TP_DICT[4][1]].copy()\n",
    "        tmpH1PET.columns = [\"R_%d\" % rR]\n",
    "        H1PET4 = H1PET4.merge( tmpH1PET, how='inner', left_index=True, right_index=True )\n",
    "    # end of if\n",
    "# end of for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFNamer = \"WBMon_H1_PET1_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H1PET1.to_pickle( OFPath, compression='zip' )\n",
    "#H1PET1 = pd.read_pickle( OFPath, compression='zip' )\n",
    "OFNamer = \"WBMon_H1_PET2_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H1PET2.to_pickle( OFPath, compression='zip' )\n",
    "#H1PET2 = pd.read_pickle( OFPath, compression='zip' )\n",
    "OFNamer = \"WBMon_H1_PET3_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H1PET3.to_pickle( OFPath, compression='zip' )\n",
    "#H1PET3 = pd.read_pickle( OFPath, compression='zip' )\n",
    "OFNamer = \"WBMon_H1_PET4_%s.pickle\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "H1PET4.to_pickle( OFPath, compression='zip' )\n",
    "#H1PET4 = pd.read_pickle( OFPath, compression='zip' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make PET Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0PET1[\"Year\"] = H0PET1.index.year\n",
    "H0PET1[\"Month\"] = H0PET1.index.month\n",
    "H0PET2[\"Year\"] = H0PET2.index.year\n",
    "H0PET2[\"Month\"] = H0PET2.index.month\n",
    "H0PET3[\"Year\"] = H0PET3.index.year\n",
    "H0PET3[\"Month\"] = H0PET3.index.month\n",
    "H0PET4[\"Year\"] = H0PET4.index.year\n",
    "H0PET4[\"Month\"] = H0PET4.index.month\n",
    "H0PET1[\"Average\"] = H0PET1.mean( axis=1 )\n",
    "H0PET2[\"Average\"] = H0PET2.mean( axis=1 )\n",
    "H0PET3[\"Average\"] = H0PET3.mean( axis=1 )\n",
    "H0PET4[\"Average\"] = H0PET4.mean( axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1PET1[\"Year\"] = H1PET1.index.year\n",
    "H1PET1[\"Month\"] = H1PET1.index.month\n",
    "H1PET2[\"Year\"] = H1PET2.index.year\n",
    "H1PET2[\"Month\"] = H1PET2.index.month\n",
    "H1PET3[\"Year\"] = H1PET3.index.year\n",
    "H1PET3[\"Month\"] = H1PET3.index.month\n",
    "H1PET4[\"Year\"] = H1PET4.index.year\n",
    "H1PET4[\"Month\"] = H1PET4.index.month\n",
    "H1PET1[\"Average\"] = H1PET1.mean( axis=1 )\n",
    "H1PET2[\"Average\"] = H1PET2.mean( axis=1 )\n",
    "H1PET3[\"Average\"] = H1PET3.mean( axis=1 )\n",
    "H1PET4[\"Average\"] = H1PET4.mean( axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0PTPET1 = H0PET1.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )\n",
    "H0PTPET2 = H0PET2.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )\n",
    "H0PTPET3 = H0PET3.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )\n",
    "H0PTPET4 = H0PET4.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1PTPET1 = H1PET1.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )\n",
    "H1PTPET2 = H1PET2.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )\n",
    "H1PTPET3 = H1PET3.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )\n",
    "H1PTPET4 = H1PET4.pivot( index=\"Year\", columns=\"Month\", values=\"Average\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFNamer = \"WSPETMon_%s_PivotTables.xlsx\" % OUT_ROOT\n",
    "OFPath = os.path.normpath( os.path.join( OUT_DIR, OFNamer ) )\n",
    "with pd.ExcelWriter( OFPath ) as writer:\n",
    "    H0PTPET1.to_excel( writer, sheet_name='H0_PET1', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H0PTPET2.to_excel( writer, sheet_name='H0_PET2', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H0PTPET3.to_excel( writer, sheet_name='H0_PET3', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H0PTPET4.to_excel( writer, sheet_name='H0_PET4', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H1PTPET1.to_excel( writer, sheet_name='H1_PET1', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H1PTPET2.to_excel( writer, sheet_name='H1_PET2', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H1PTPET3.to_excel( writer, sheet_name='H1_PET3', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "    H1PTPET4.to_excel( writer, sheet_name='H1_PET4', na_rep=str(np.nan),\n",
    "                        index_label=\"Year\" )\n",
    "# end of with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time History Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DictH0PET1 = dict()\n",
    "DictH0PET2 = dict()\n",
    "DictH0PET3 = dict()\n",
    "DictH0PET4 = dict()\n",
    "hdrCnt = 0\n",
    "# start out by doing the percentiles\n",
    "for iI in range(NumQs):\n",
    "    curQ = OurQs[iI]\n",
    "    DictH0PET1[StatsHdsList[hdrCnt]] = H0PET1.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    DictH0PET2[StatsHdsList[hdrCnt]] = H0PET2.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    DictH0PET3[StatsHdsList[hdrCnt]] = H0PET3.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    DictH0PET4[StatsHdsList[hdrCnt]] = H0PET4.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    hdrCnt += 1\n",
    "# end for\n",
    "DictH0PET1[StatsHdsList[hdrCnt]] = H0PET1.mean( axis=1 ).to_numpy()\n",
    "DictH0PET2[StatsHdsList[hdrCnt]] = H0PET2.mean( axis=1 ).to_numpy()\n",
    "DictH0PET3[StatsHdsList[hdrCnt]] = H0PET3.mean( axis=1 ).to_numpy()\n",
    "DictH0PET4[StatsHdsList[hdrCnt]] = H0PET4.mean( axis=1 ).to_numpy()\n",
    "hdrCnt += 1\n",
    "DictH0PET1[StatsHdsList[hdrCnt]] = H0PET1.var( axis=1 ).to_numpy()\n",
    "DictH0PET2[StatsHdsList[hdrCnt]] = H0PET2.var( axis=1 ).to_numpy()\n",
    "DictH0PET3[StatsHdsList[hdrCnt]] = H0PET3.var( axis=1 ).to_numpy()\n",
    "DictH0PET4[StatsHdsList[hdrCnt]] = H0PET4.var( axis=1 ).to_numpy()\n",
    "hdrCnt += 1\n",
    "DictH0PET1[StatsHdsList[hdrCnt]] = H0PET1.skew( axis=1 ).to_numpy()\n",
    "DictH0PET2[StatsHdsList[hdrCnt]] = H0PET2.skew( axis=1 ).to_numpy()\n",
    "DictH0PET3[StatsHdsList[hdrCnt]] = H0PET3.skew( axis=1 ).to_numpy()\n",
    "DictH0PET4[StatsHdsList[hdrCnt]] = H0PET4.skew( axis=1 ).to_numpy()\n",
    "hdrCnt += 1\n",
    "DictH0PET1[StatsHdsList[hdrCnt]] = H0PET1.kurt( axis=1 ).to_numpy()\n",
    "DictH0PET2[StatsHdsList[hdrCnt]] = H0PET2.kurt( axis=1 ).to_numpy()\n",
    "DictH0PET3[StatsHdsList[hdrCnt]] = H0PET3.kurt( axis=1 ).to_numpy()\n",
    "DictH0PET4[StatsHdsList[hdrCnt]] = H0PET4.kurt( axis=1 ).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimIndex1 = H0PET1.index\n",
    "TimIndex2 = H0PET2.index\n",
    "TimIndex3 = H0PET3.index\n",
    "TimIndex4 = H0PET4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0PETStats1 = pd.DataFrame(index=TimIndex1, data=DictH0PET1 )\n",
    "H0PETStats2 = pd.DataFrame(index=TimIndex2, data=DictH0PET2 )\n",
    "H0PETStats3 = pd.DataFrame(index=TimIndex3, data=DictH0PET3 )\n",
    "H0PETStats4 = pd.DataFrame(index=TimIndex4, data=DictH0PET4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DictH1PET1 = dict()\n",
    "DictH1PET2 = dict()\n",
    "DictH1PET3 = dict()\n",
    "DictH1PET4 = dict()\n",
    "hdrCnt = 0\n",
    "# start out by doing the percentiles\n",
    "for iI in range(NumQs):\n",
    "    curQ = OurQs[iI]\n",
    "    DictH1PET1[StatsHdsList[hdrCnt]] = H1PET1.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    DictH1PET2[StatsHdsList[hdrCnt]] = H1PET2.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    DictH1PET3[StatsHdsList[hdrCnt]] = H1PET3.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    DictH1PET4[StatsHdsList[hdrCnt]] = H1PET4.quantile( q=curQ, axis=1 ).to_numpy()\n",
    "    hdrCnt += 1\n",
    "# end for\n",
    "DictH1PET1[StatsHdsList[hdrCnt]] = H1PET1.mean( axis=1 ).to_numpy()\n",
    "DictH1PET2[StatsHdsList[hdrCnt]] = H1PET2.mean( axis=1 ).to_numpy()\n",
    "DictH1PET3[StatsHdsList[hdrCnt]] = H1PET3.mean( axis=1 ).to_numpy()\n",
    "DictH1PET4[StatsHdsList[hdrCnt]] = H1PET4.mean( axis=1 ).to_numpy()\n",
    "hdrCnt += 1\n",
    "DictH1PET1[StatsHdsList[hdrCnt]] = H1PET1.var( axis=1 ).to_numpy()\n",
    "DictH1PET2[StatsHdsList[hdrCnt]] = H1PET2.var( axis=1 ).to_numpy()\n",
    "DictH1PET3[StatsHdsList[hdrCnt]] = H1PET3.var( axis=1 ).to_numpy()\n",
    "DictH1PET4[StatsHdsList[hdrCnt]] = H1PET4.var( axis=1 ).to_numpy()\n",
    "hdrCnt += 1\n",
    "DictH1PET1[StatsHdsList[hdrCnt]] = H1PET1.skew( axis=1 ).to_numpy()\n",
    "DictH1PET2[StatsHdsList[hdrCnt]] = H1PET2.skew( axis=1 ).to_numpy()\n",
    "DictH1PET3[StatsHdsList[hdrCnt]] = H1PET3.skew( axis=1 ).to_numpy()\n",
    "DictH1PET4[StatsHdsList[hdrCnt]] = H1PET4.skew( axis=1 ).to_numpy()\n",
    "hdrCnt += 1\n",
    "DictH1PET1[StatsHdsList[hdrCnt]] = H1PET1.kurt( axis=1 ).to_numpy()\n",
    "DictH1PET2[StatsHdsList[hdrCnt]] = H1PET2.kurt( axis=1 ).to_numpy()\n",
    "DictH1PET3[StatsHdsList[hdrCnt]] = H1PET3.kurt( axis=1 ).to_numpy()\n",
    "DictH1PET4[StatsHdsList[hdrCnt]] = H1PET4.kurt( axis=1 ).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1PETStats1 = pd.DataFrame(index=TimIndex1, data=DictH1PET1 )\n",
    "H1PETStats2 = pd.DataFrame(index=TimIndex2, data=DictH1PET2 )\n",
    "H1PETStats3 = pd.DataFrame(index=TimIndex3, data=DictH1PET3 )\n",
    "H1PETStats4 = pd.DataFrame(index=TimIndex4, data=DictH1PET4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFileName = \"WBMon_PET_%s_Stats.xlsx\" % OUT_ROOT\n",
    "OutFP = os.path.normpath( os.path.join( OUT_DIR, OutFileName ) )\n",
    "with pd.ExcelWriter( OutFP ) as writer:\n",
    "    H0PETStats1.to_excel( writer, sheet_name=\"H0PET_1\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H0PETStats2.to_excel( writer, sheet_name=\"H0PET_2\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H0PETStats3.to_excel( writer, sheet_name=\"H0PET_3\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H0PETStats4.to_excel( writer, sheet_name=\"H0PET_4\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H1PETStats1.to_excel( writer, sheet_name=\"H1PET_1\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H1PETStats2.to_excel( writer, sheet_name=\"H1PET_2\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H1PETStats3.to_excel( writer, sheet_name=\"H1PET_3\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "    H1PETStats4.to_excel( writer, sheet_name=\"H1PET_4\", index_label=\"Date\",\n",
    "                        columns=StatsHdsList, na_rep=str(np.nan) )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
